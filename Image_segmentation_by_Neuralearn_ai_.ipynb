{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhas10bc/tensor_flow_course/blob/main/Image_segmentation_by_Neuralearn_ai_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdV7TJbUP-hy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf### models\n",
        "import numpy as np### math computations\n",
        "import matplotlib.pyplot as plt### plotting bar chart\n",
        "import sklearn### machine learning library\n",
        "import cv2## image processing\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import (GlobalAveragePooling2D, Activation, MaxPooling2D, Add, Conv2D, MaxPool2D, Dense,\n",
        "                                     Flatten, InputLayer, BatchNormalization, Input, Embedding, Permute,\n",
        "                                     Dropout, RandomFlip, RandomRotation, LayerNormalization, MultiHeadAttention,\n",
        "                                     RandomContrast, Rescaling, Resizing, Reshape,LeakyReLU)\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import (Callback, CSVLogger, EarlyStopping, LearningRateScheduler,\n",
        "                                        ModelCheckpoint, ReduceLROnPlateau)\n",
        "from tensorflow.keras.regularizers import L2, L1\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Example, Features, Feature\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d2IMnk1bUIe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPARATION"
      ],
      "metadata": {
        "id": "hy68yrYIULL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__ (self, images, maps, batch_size, INPUT_DIM, shuffle = False):\n",
        "\n",
        "        self.images = images\n",
        "        self.maps = maps\n",
        "        self.batch_size = batch_size\n",
        "        self.train_image_list=os.listdir(images)\n",
        "        self.INPUT_DIM=INPUT_DIM\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.train_image_list)/self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x,y = self.__data_generation(idx)\n",
        "        y-=1\n",
        "        return np.array(x), np.array(y)\n",
        "\n",
        "    def __data_generation(self, idx):\n",
        "        x = []\n",
        "        y = []\n",
        "\n",
        "        for j in range(idx*self.batch_size, (idx+1)*self.batch_size):\n",
        "            x.append(img_to_array(load_img(self.images+os.listdir(self.images)[j],target_size=(self.INPUT_DIM,self.INPUT_DIM))))\n",
        "            y.append(img_to_array(load_img(self.maps+os.listdir(self.maps)[j], color_mode='grayscale', target_size=(self.INPUT_DIM//2,self.INPUT_DIM//2))))\n",
        "\n",
        "    return tf.convert_to_tensor(x),tf.convert_to_tensor(y)"
      ],
      "metadata": {
        "id": "jjup7DNIUIhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images='...'\n",
        "train_maps='...'\n",
        "val_images='...'\n",
        "val_maps='...'\n",
        "\n",
        "LR=1e-3\n",
        "BATCH_SIZE=4\n",
        "EPOCH=100"
      ],
      "metadata": {
        "id": "Zu2qO_-nUIjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = DataGenerator(images, maps,BATCH_SIZE,INPUT_DIM)\n",
        "val_gen = DataGenerator(val_images, val_maps,BATCH_SIZE,INPUT_DIM)"
      ],
      "metadata": {
        "id": "Aoz9frgoUImT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELING"
      ],
      "metadata": {
        "id": "I2SBs5aLUPT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_base_model():\n",
        "    base_model = tf.keras.applications.ResNet50(\n",
        "        weights='imagenet',\n",
        "        input_shape=(INPUT_DIM,INPUT_DIM,3),\n",
        "        include_top=False,)\n",
        "    base_model.trainable=False\n",
        "\n",
        "    conv1_relu,conv2_block3_out,conv3_block4_out,conv4_block6_out,conv5_block3_out=[base_model.get_layer(layer_name).output for layer_name in [\"conv1_relu\",\"conv2_block3_out\",\"conv3_block4_out\",\"conv4_block6_out\",\"conv5_block3_out\"]]\n",
        "\n",
        "    return Model(\n",
        "        inputs=[base_model.inputs],outputs=[conv1_relu,conv2_block3_out,conv3_block4_out,conv4_block6_out,conv5_block3_out]\n",
        "    )\n",
        "\n",
        "get_base_model().summary()"
      ],
      "metadata": {
        "id": "NN-LGUw5UIo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Upsample(tf.keras.layers.Layer):\n",
        "    def __init__(self,NUM_FILTERS):\n",
        "        super(Upsample,self).__init__()\n",
        "        self.conv_t_1=Conv2DTranspose(NUM_FILTERS,1,strides=2,activation='relu')\n",
        "        self.norm_1=BatchNormalization()\n",
        "    def call(self,x):\n",
        "        x=self.norm_1(self.conv_t_1(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "YOI1J0PSUItT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayers(tf.keras.layers.Layer):\n",
        "    def __init__(self,NUM_FILTERS):\n",
        "        super(ConvLayers,self).__init__()\n",
        "        self.conv_1=Conv2D(NUM_FILTERS*2,3,padding='same',activation='relu')\n",
        "        self.norm_1=BatchNormalization()\n",
        "\n",
        "        self.conv_2=Conv2D(NUM_FILTERS*4,3,padding='same',activation='relu')\n",
        "        self.norm_2=BatchNormalization()\n",
        "    def call(self,x):\n",
        "        x=self.norm_1(self.conv_1(x))\n",
        "        x=self.norm_2(self.conv_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "kKLEmdqSUIvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=tf.keras.Input(shape=(INPUT_DIM,INPUT_DIM,3))\n",
        "x=Rescaling(1/255.)(inputs)\n",
        "x_112,x_56,x_28,x_7=get_base_model()(x)\n",
        "\n",
        "x=Upsample(NUM_FILTERS)(x_7)\n",
        "x=tf.concat([x,x_14],axis=-1)\n",
        "x=ConvLayers(NUM_FILTERS)(x)\n",
        "\n",
        "\n",
        "x=Upsample(NUM_FILTERS)(x)\n",
        "x=tf.concat([x,x_28],axis=-1)\n",
        "x=ConvLayers(NUM_FILTERS)(x)\n",
        "\n",
        "\n",
        "x=Upsample(NUM_FILTERS)(x)\n",
        "x=tf.concat([x,x_56],axis=-1)\n",
        "x=ConvLayers(NUM_FILTERS)(x)\n",
        "\n",
        "\n",
        "x=Upsample(NUM_FILTERS)(x)\n",
        "x=tf.concat([x,x_112],axis=-1)\n",
        "x=ConvLayers(NUM_FILTERS)(x)\n",
        "\n",
        "out=Conv2D(3,3,padding='same',activation='softmax')(x)\n",
        "model=tf.keras.Model(inputs=inputs,outputs=out)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "WaVhwW9BUV11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING"
      ],
      "metadata": {
        "id": "qiHbxjJqUZpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer = Adam(learning_rate = LR),\n",
        "    metrics='accuracy',\n",
        "    run_eagerly = True,\n",
        ")"
      ],
      "metadata": {
        "id": "j4y4mE1fUV4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath='...'\n",
        "callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "Nu53xe4PUV6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_gen,\n",
        "    verbose=1,\n",
        "    shuffle=True,\n",
        "    epochs=EPOCH,\n",
        "    callbacks=[callback])"
      ],
      "metadata": {
        "id": "a3LCC5dcUdey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING"
      ],
      "metadata": {
        "id": "AfWOZFeIUe6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image ='...'\n",
        "test_image_map='...'\n",
        "\n",
        "X=[]\n",
        "X.append(img_to_array(load_img(test_image,target_size=(224,224))))\n",
        "image_output=tf.argmax(model.predict(tf.constant(X)),axis=-1)[0]\n",
        "image_output=tf.expand_dims(image_output,axis=-1)"
      ],
      "metadata": {
        "id": "yGiqkCrUUfn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image=tf.keras.preprocessing.image.load_img(test_image,color_mode='rgb',target_size=(224,224))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SelZupaeUfqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image_output[...,0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3jf35O9QUfs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wz_yrr2TUfvz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}